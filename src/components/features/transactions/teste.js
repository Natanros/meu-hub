/**
 * ðŸŽ¤ Whisper Audio Transcriber
 * Captura Ã¡udio do microfone e transcreve usando Whisper (Hugging Face Transformers.js)
 */

import { pipeline } from "@huggingface/transformers";

// Classe para gerenciar a transcriÃ§Ã£o de Ã¡udio com Whisper
export class WhisperAudioTranscriber {
  constructor() {
    this.transcriber = null;
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isRecording = false;
  }

  // Inicializar o pipeline do Whisper (lazy loading)
  async initializeTranscriber() {
    if (!this.transcriber) {
      console.log("ðŸ”„ Carregando modelo Whisper...");
      // Usando modelo em portuguÃªs para melhor precisÃ£o
      this.transcriber = await pipeline(
        "automatic-speech-recognition",
        "Xenova/whisper-tiny" // Modelo multilÃ­ngue, suporta portuguÃªs
      );
      console.log("âœ… Whisper carregado com sucesso!");
    }
    return this.transcriber;
  }

  // Iniciar gravaÃ§Ã£o de Ã¡udio
  async startRecording() {
    try {
      // Verificar se getUserMedia estÃ¡ disponÃ­vel
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error(
          "Seu navegador nÃ£o suporta gravaÃ§Ã£o de Ã¡udio. Por favor, use HTTPS ou um navegador mais recente."
        );
      }

      // Solicitar acesso ao microfone
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000, // Whisper espera 16kHz
          channelCount: 1, // Mono
          echoCancellation: true,
          noiseSuppression: true,
        },
      });

      this.audioChunks = [];

      // Criar MediaRecorder
      this.mediaRecorder = new MediaRecorder(stream, {
        mimeType: "audio/webm;codecs=opus",
      });

      // Coletar chunks de Ã¡udio
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.audioChunks.push(event.data);
        }
      };

      this.mediaRecorder.start();
      this.isRecording = true;
      console.log("ðŸŽ¤ GravaÃ§Ã£o iniciada...");

      return true;
    } catch (error) {
      console.error("âŒ Erro ao acessar microfone:", error);

      // Mensagens de erro mais especÃ­ficas
      if (error.name === "NotAllowedError") {
        throw new Error(
          "PermissÃ£o para acessar o microfone foi negada. Por favor, permita o acesso ao microfone."
        );
      } else if (error.name === "NotFoundError") {
        throw new Error("Nenhum microfone foi encontrado no dispositivo.");
      } else if (error.name === "NotSupportedError") {
        throw new Error(
          "GravaÃ§Ã£o de Ã¡udio nÃ£o Ã© suportada neste navegador. Use HTTPS."
        );
      } else if (error.message.includes("getUserMedia")) {
        throw new Error(
          "GravaÃ§Ã£o de Ã¡udio sÃ³ funciona em conexÃµes HTTPS ou localhost."
        );
      }

      throw error;
    }
  }

  // Parar gravaÃ§Ã£o e retornar Ã¡udio
  async stopRecording() {
    return new Promise((resolve, reject) => {
      if (!this.mediaRecorder || !this.isRecording) {
        reject(new Error("GravaÃ§Ã£o nÃ£o iniciada"));
        return;
      }

      this.mediaRecorder.onstop = async () => {
        try {
          // Criar blob do Ã¡udio
          const audioBlob = new Blob(this.audioChunks, { type: "audio/webm" });

          // Parar todas as tracks de mÃ­dia
          this.mediaRecorder.stream
            .getTracks()
            .forEach((track) => track.stop());

          this.isRecording = false;
          console.log("â¹ï¸ GravaÃ§Ã£o finalizada");

          resolve(audioBlob);
        } catch (error) {
          reject(error);
        }
      };

      this.mediaRecorder.stop();
    });
  }

  // Converter Ã¡udio para o formato esperado pelo Whisper
  async processAudioBlob(audioBlob) {
    try {
      // Converter blob para ArrayBuffer
      const arrayBuffer = await audioBlob.arrayBuffer();

      // Usar AudioContext para processar
      const audioContext = new (window.AudioContext ||
        window.webkitAudioContext)({
        sampleRate: 16000,
      });

      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

      // Extrair dados de Ã¡udio (canal 0 = mono ou primeiro canal)
      let audioData = audioBuffer.getChannelData(0);

      // Se tiver mÃºltiplos canais, fazer merge
      if (audioBuffer.numberOfChannels > 1) {
        const SCALING_FACTOR = Math.sqrt(2);
        const channel1 = audioBuffer.getChannelData(0);
        const channel2 = audioBuffer.getChannelData(1);

        audioData = new Float32Array(channel1.length);
        for (let i = 0; i < channel1.length; i++) {
          audioData[i] = (SCALING_FACTOR * (channel1[i] + channel2[i])) / 2;
        }
      }

      return audioData;
    } catch (error) {
      console.error("âŒ Erro ao processar Ã¡udio:", error);
      throw error;
    }
  }

  // Transcrever Ã¡udio capturado
  async transcribe(audioBlob) {
    try {
      // Garantir que o transcriber estÃ¡ inicializado
      await this.initializeTranscriber();

      console.log("ðŸ”„ Processando Ã¡udio...");
      const start = performance.now();

      // Processar Ã¡udio
      const audioData = await this.processAudioBlob(audioBlob);

      console.log("ðŸ”„ Transcrevendo com Whisper...");
      // Transcrever com Whisper
      const output = await this.transcriber(audioData, {
        language: "portuguese", // ForÃ§ar portuguÃªs
        task: "transcribe",
      });

      const end = performance.now();
      console.log(
        `âœ… TranscriÃ§Ã£o concluÃ­da em ${((end - start) / 1000).toFixed(2)}s`
      );
      console.log("ðŸ“ Texto:", output.text);

      return output.text;
    } catch (error) {
      console.error("âŒ Erro na transcriÃ§Ã£o:", error);
      throw error;
    }
  }

  // MÃ©todo completo: gravar e transcrever
  async recordAndTranscribe(onTranscriptionUpdate) {
    try {
      // Inicializar modelo em paralelo com a gravaÃ§Ã£o
      const initPromise = this.initializeTranscriber();

      await this.startRecording();

      // Aguardar inicializaÃ§Ã£o do modelo
      await initPromise;

      // Retornar funÃ§Ãµes de controle
      return {
        stop: async () => {
          const audioBlob = await this.stopRecording();
          const transcription = await this.transcribe(audioBlob);
          if (onTranscriptionUpdate) {
            onTranscriptionUpdate(transcription);
          }
          return transcription;
        },
        isRecording: () => this.isRecording,
      };
    } catch (error) {
      console.error("âŒ Erro no processo de gravaÃ§Ã£o/transcriÃ§Ã£o:", error);
      throw error;
    }
  }

  // Transcrever arquivo de Ã¡udio (URL ou Blob)
  async transcribeFromFile(audioSource) {
    try {
      await this.initializeTranscriber();

      let audioBlob;

      if (typeof audioSource === "string") {
        // Ã‰ uma URL
        const response = await fetch(audioSource);
        audioBlob = await response.blob();
      } else {
        // JÃ¡ Ã© um Blob/File
        audioBlob = audioSource;
      }

      return await this.transcribe(audioBlob);
    } catch (error) {
      console.error("âŒ Erro ao transcrever arquivo:", error);
      throw error;
    }
  }
}

// Singleton para uso global
let globalTranscriber = null;

export const getWhisperTranscriber = () => {
  if (!globalTranscriber) {
    globalTranscriber = new WhisperAudioTranscriber();
  }
  return globalTranscriber;
};
